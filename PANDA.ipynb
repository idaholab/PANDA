{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PANDA: Predictive Automation of Novel Defect Anomalies\n",
    "\n",
    "Copyright 2024, Battelle Energy Alliance, LLC, ALL RIGHTS RESERVED\n",
    "\n",
    "This program (Program) utilizes YOLOv8 under GNU Affero General Public License v3.0 (AGPL-3.0). For more information about YOLOv8, see <https://docs.ultralytics.com/models/yolov8/>. For more information about the AGPL-3.0 license, see https://gnu.org/licenses/. You should have received a copy of the GNU AGPL-3.0 license along with this Program. If not, you may find a copy of it at <https://www.gnu.org/licenses/agpl-3.0.en.html#license-text>.\n",
    "\n",
    " \n",
    "\n",
    "The YOLOv8 is free software. You can redistribute it and/or modify it under the terms of GNU AGPL-3.0 as published by the Free Software Foundation, either version 3 or the License or (at your option) any later version.\n",
    "\n",
    " \n",
    "\n",
    "The portion of the Program that is not YOLOv8 is owned by Battelle Energy Alliance (BEA), (Copyright 2024 BEA) and the source code or instruction sets for running this portion of the program, along with the source code for YOLOv8, are made available to the user upon running the Program. This Program (including the YOLOv8 portion and the BEA portion) is licensed to the user under AGPL-3.0 and can be used according to that license for so long as the user is in compliance with that license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and version numbers tested: \n",
    "python, version number: 3.11.5  \n",
    "opencv-python, version number: 4.9.0.80  \n",
    "numpy, version number: 1.26.3  \n",
    "shapely, version number: 2.0.2  \n",
    "matplotlib, version number: 3.8.2   \n",
    "ultralytics, version number: 8.0.234  \n",
    "scikit-learn, version number: 1.3.2  \n",
    " PyYAML, version number: 6.0.1  \n",
    "tensorflow, version number: 2.15.0.post1  \n",
    "pillow, version number: 10.2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation and Folder Structure\n",
    "\n",
    "Begin by downloading `nature_img_used.zip` into the same directory as this code, and unzipping it. In it you will find three folders and a YAML file.\n",
    "\n",
    "\n",
    "`original_images` contains the raw images obtained from other studies before pre-processing is applied. \n",
    "`artery_img_used` includes images and labels from `\"Dataset for Automatic Region-based Coronary Artery Disease Diagnostics Using X-Ray Angiography Images\"`. Note that this dataset includes multiple classes of arteries that  will be changed to all be 0 aka `loop`. Only 39 images were randomly selected from this dataset to match the number from the loop-heavy `Shen et al.`.\n",
    "\n",
    "`shen_img_used_loops` includes two images from `\"Multi defect detection and analysis of electron microscopy images with deep learning\"` and will be referred to elsewhere in this code as `Shen et al.`. The two images are further separated into their own sub-directories due to the differing augmentations applied to them.\n",
    "\n",
    "`ma956_ods_alloy_dataset` includes images that were obtained as part of NSUF-CINR-18-14787. These images are of a commercially used MA956 ODS alloy that was irradiated. Post irradiation characterization was carried out at Microscopy and Characterization Suite (MaCS), Center of Advanced Energy Studies (CAES). \n",
    "\n",
    "The labels of both `shen_img_used_loops` and `ma956_ods_alloy_dataset` were manually annotated during this study using Label Studio, and exported to YOLO format. The original ground-truth labels of `Shen et al.` used a format incompatible with training on YOLO, and thus were re-labelled by this study's researchers. Due to the limited number of annotated images, augmentation techniques are used to create more training images from this select number.\n",
    "\n",
    "\n",
    "The test images in `test_img` are not trained on, and are predicted on by the trained models for analysis. \n",
    "\n",
    "The same goes for `shen_test` which are images from Shen et al. that are not trained on.\n",
    "\n",
    "Both folders also include the respective ground truth label .txt files. Note that `shen_test` images were not re-labelled, and include the original label format from that study.\n",
    "\n",
    "Those can be found in`DataSetFinal.zip` which can be downloaded from: `https://figshare.com/articles/dataset/Data_for_Multi_Defect_Detection_and_Analysis_of_Electron_Microscopy_Images_with_Deep_Learning_/8266484`.  \n",
    "\n",
    "\n",
    "`nature_models.yaml` is a configuration file that defines the directories, classes, and image augmentations used during training.\n",
    "##### Do note that YAML files only accept direct PATHs, and thus you must fill in your working directory (aka the directory where you unzipped nature_img_used.zip) at the beginning of the `path` parameters in the file.\n",
    "##### Only one of the two `path` parameters should remain uncommented for a given training, and they should be used when training Model A and B, and the artery_shen_model, respectively.\n",
    "\n",
    "The weight files for the Artery and Shen Model, Model A, and Model B, have been included as artery_shen_weights.pt, model_a.pt, and model_b.pt, respectively, for the exact weights used in the paper. These can be used in Function 7 to output the exact same figures shown in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- [**Function 1**](#function1)  \n",
    "  Augment Image Contrast\n",
    "\n",
    "- [**Function 2**](#function2)  \n",
    "  Divide Images and Labels into Multiple Subsections\n",
    "  \n",
    "- [**Function 3**](#function3)  \n",
    "  Combine Artery Dataset Classes\n",
    "  \n",
    "- [**Function 4**](#function4)  \n",
    "  Resize Input Images\n",
    "  \n",
    "- [**Function 5**](#function5)  \n",
    "  Split Data into Training and Validation Sets\n",
    "  \n",
    "- [**Function 6**](#function6)  \n",
    "  Train Models A and B\n",
    "\n",
    "- [**Function 7**](#function7)  \n",
    "  Predict with Models\n",
    "  \n",
    "- [**Function 8**](#function8)  \n",
    "  Generate Mask Images\n",
    "  \n",
    "- [**Function 9**](#function9)  \n",
    "  Calculate Metrics from Binary Masks\n",
    "  \n",
    "- [**Function 10**](#function10)  \n",
    "  Calculate Noise Level of Images\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment Image Contrast (Function 1) <a class=\"anchor\" id=\"function1\"></a>\n",
    "\n",
    "**Purpose**  \n",
    "Changes the contrast of the inputed images to augment the training data. Only the `200kV_500kx_p2nm_8cmCL_grain1_0111` image is augmented, as it is significantly more loop-heavy than the other `Shen et al.` image re-labelled.\n",
    "\n",
    "\n",
    "**Parameters**  \n",
    "`img_dir`: The original image directory.\n",
    "\n",
    "`contrast_dir`: The directory to save the images with altered contrast.\n",
    "\n",
    "`label_dir`: The original label directory.\n",
    "\n",
    "`contrast_label_dir`: The directory to save the labels corresponding to the new images with altered contrast. These labels are identical to the originals, but are renamed to correspond to the new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "def augment_image_contrast(img_dir, contrast_dir, label_dir, contrast_label_dir):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(contrast_dir, exist_ok=True)\n",
    "    os.makedirs(contrast_label_dir, exist_ok=True)\n",
    "    \n",
    "    # Get a list of all image files in the directory\n",
    "    img_files = [f for f in os.listdir(img_dir) if f.endswith('.tif') or f.endswith('.png') or f.endswith('.jpg')]\n",
    "    for img_file in img_files:\n",
    "        # Set paths\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        filename, file_extension = os.path.splitext(img_file)\n",
    "        contrast_path= os.path.join(contrast_dir, f'{filename}-darkened.{file_extension}')\n",
    "        \n",
    "        # Copy label file to new path to correspond with new image\n",
    "        label_path = os.path.join(label_dir, f'{filename}.txt')\n",
    "        contrast_label_path = os.path.join(contrast_label_dir, f'{filename}-darkened.txt')\n",
    "        shutil.copyfile(label_path, contrast_label_path)\n",
    "        \n",
    "        # Adjust contrast via cv2, adapted from https://docs.opencv.org/4.x/d3/dc1/tutorial_basic_linear_transform.html\n",
    "        img = cv2.imread(img_path)\n",
    "        contrast_img = np.zeros(img.shape, img.dtype)\n",
    "        \n",
    "        for y in range(img.shape[0]):\n",
    "            for x in range(img.shape[1]):\n",
    "                for c in range(img.shape[2]):\n",
    "                    contrast_img[y,x,c] = np.clip(2.0*img[y,x,c] - 200, 0, 255)\n",
    "        \n",
    "        cv2.imwrite(contrast_path, contrast_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_image_contrast(\"./nature_img_used/original_images/shen/200kV_500kx_p2nm_8cmCL_grain1_0111/\", \"./nature_img_used/original_images/shen/200kV_500kx_p2nm_8cmCL_grain1_0111/\", \n",
    "                       \"./nature_img_used/original_images/shen/200kV_500kx_p2nm_8cmCL_grain1_0111/\", \"./nature_img_used/original_images/shen/200kV_500kx_p2nm_8cmCL_grain1_0111/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide Images and Labels into Multiple Subsections (Function 2) <a class=\"anchor\" id=\"function2\"></a>\n",
    "\n",
    "**Purpose**  \n",
    "Divides the images and corresponding labels downloaded from their sources into multiple smaller subsections to be used in training. This helps augment the training by generating more images to train on. The `200kV_500kx_p2nm_8cmCL_grain1_0111` image from `Shen et al.` is divided into the most training images due to being significantly more loop-heavy than the other Shen et al. image re-labelled, and the `MA956` images are divided into fewer as there are more images labelled in the YOLO format to train on to begin with. \n",
    "\n",
    "\n",
    "**Parameters**  \n",
    "`img_dir`: The original image directory.  \n",
    "\n",
    "`label_dir`: The label directory. The labels must be in YOLO format.  \n",
    "\n",
    "`divided_img_dir`: The output directory to save the divided images.\n",
    "\n",
    "`divided_label_dir`: The output directory to save the divided labels.\n",
    "\n",
    "`dimension`: The number of vertical and horizontal segments to divide each image into (e.g. 3 would split the images into 3x3 segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def divide_images_and_labels(img_dir, label_dir, divided_img_dir, divided_label_dir, dimension):\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(divided_img_dir, exist_ok=True)\n",
    "    os.makedirs(divided_label_dir, exist_ok=True)\n",
    "\n",
    "    divide_ratio = dimension  # Ratio to divide image into x-by-x (e.g., 2x2 grid)\n",
    "\n",
    "    # Get a list of all image files in the directory\n",
    "    img_files = [f for f in os.listdir(img_dir) if f.endswith('.tif') or f.endswith('.png') or f.endswith('.jpg')]\n",
    "\n",
    "    # Iterate through each image file\n",
    "    for img_file in img_files:\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        label_path = os.path.join(label_dir, f'{os.path.splitext(img_file)[0]}.txt')\n",
    "        \n",
    "        # Add full image/label to the directory to train on too\n",
    "        output_img_path = os.path.join(divided_img_dir, img_file)\n",
    "        output_label_path = os.path.join(divided_label_dir, f'{os.path.splitext(img_file)[0]}.txt')\n",
    "        shutil.copyfile(img_path, output_img_path)\n",
    "        shutil.copyfile(label_path, output_label_path)\n",
    "\n",
    "        # Read the image\n",
    "        img = cv2.imread(img_path)\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        # Define divide size (e.g., half of the width and height for a 2x2 grid)\n",
    "        divide_width, divide_height = w // divide_ratio, h // divide_ratio\n",
    "\n",
    "        # Read labels from the corresponding label file\n",
    "        with open(label_path, 'r') as f:\n",
    "            labels = f.read().splitlines()\n",
    "\n",
    "        # Process each divide\n",
    "        for i in range(divide_ratio):  # For horizontal splits\n",
    "            for j in range(divide_ratio):  # For vertical splits\n",
    "                x_offset, y_offset = i * divide_width, j * divide_height\n",
    "                divide = img[y_offset:y_offset + divide_height, x_offset:x_offset + divide_width]\n",
    "\n",
    "                # Process labels for this divide\n",
    "                divided_labels = []\n",
    "                for label in labels:\n",
    "                    class_id, *poly = label.split(' ')\n",
    "                    poly = np.asarray(poly, dtype=np.float32).reshape(-1, 2)\n",
    "                    poly *= [w, h]\n",
    "\n",
    "                    # Clip a polygon to the boundaries of a divide area defined by x_offset, y_offset, divide_width, divide_height\n",
    "                    # Create a polygon representing the divide area\n",
    "                    divide_rect = Polygon([(x_offset, y_offset), (x_offset + divide_width, y_offset), \n",
    "                                         (x_offset + divide_width, y_offset + divide_height), (x_offset, y_offset + divide_height)])\n",
    "\n",
    "                    # Create a polygon from the input polygon coordinates\n",
    "                    label_poly = Polygon(poly.reshape(-1, 2))\n",
    "\n",
    "                    # Fix the polygon if it's invalid\n",
    "                    if not label_poly.is_valid:\n",
    "                        label_poly = label_poly.buffer(0)\n",
    "\n",
    "                    # Calculate the intersection of the label polygon with the divide area\n",
    "                    clipped_poly = label_poly.intersection(divide_rect)\n",
    "\n",
    "                    # If the intersection is not empty, return the coordinates of the clipped polygon\n",
    "                    if not clipped_poly.is_empty:\n",
    "                        if clipped_poly.geom_type == 'MultiPolygon':\n",
    "                            # Handle MultiPolygon case by concatenating coordinates of all polygons\n",
    "                            coords = np.concatenate([np.array(poly.exterior.coords.xy).T for poly in clipped_poly.geoms])\n",
    "                        else:\n",
    "                            # Single Polygon case, return its coordinates\n",
    "                            coords = np.array(clipped_poly.exterior.coords.xy).T\n",
    "                        clipped_poly = coords\n",
    "                    else: \n",
    "                        clipped_poly = np.array([]).reshape(0, 2)\n",
    "\n",
    "                    if clipped_poly.size > 0:\n",
    "                        # Adjust polygon coordinates relative to a divide area and convert them to ratios\n",
    "                        # Adjust the polygon coordinates to the divided image's coordinate system\n",
    "                        adjusted_poly = clipped_poly.copy()\n",
    "                        adjusted_poly[:, 0] -= x_offset\n",
    "                        adjusted_poly[:, 1] -= y_offset\n",
    "\n",
    "                        # Convert the coordinates to ratios relative to the divided image\n",
    "                        adjusted_poly[:, 0] = np.clip(adjusted_poly[:, 0] / divide_width, 0, 1)\n",
    "                        adjusted_poly[:, 1] = np.clip(adjusted_poly[:, 1] / divide_height, 0, 1)\n",
    "\n",
    "                        # Save polygons as ratios in the label file\n",
    "                        divided_labels.append(f\"{class_id} {' '.join(map(str, adjusted_poly.flatten()))}\")\n",
    "\n",
    "                # Save the divided image\n",
    "                divided_img_filename = f\"{os.path.splitext(img_file)[0]}_{i}_{j}.png\"\n",
    "                divided_img_path = os.path.join(divided_img_dir, divided_img_filename)\n",
    "                cv2.imwrite(divided_img_path, divide)\n",
    "\n",
    "                # Save the divided label\n",
    "                divided_label_filename = f\"{os.path.splitext(img_file)[0]}_{i}_{j}.txt\"\n",
    "                divided_label_path = os.path.join(divided_label_dir, divided_label_filename)\n",
    "                with open(divided_label_path, 'w') as f:\n",
    "                    for divided_label in divided_labels:\n",
    "                        f.write(f\"{divided_label}\\n\")\n",
    "\n",
    "    print(\"Images and labels are divided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divides the Shen et al. images\n",
    "divide_images_and_labels(\"./nature_img_used/original_images/shen/1ROI_100kx_4100CL_foil1/\", \"./nature_img_used/original_images/shen/1ROI_100kx_4100CL_foil1\", \"./nature_img_used/augmented/shen_divided/images/\", \"./nature_img_used/augmented/shen_divided/labels/\", 2)\n",
    "divide_images_and_labels(\"./nature_img_used/original_images/shen/200kV_500kx_p2nm_8cmCL_grain1_0111/\", \"./nature_img_used/original_images/shen/200kV_500kx_p2nm_8cmCL_grain1_0111/\", \"./nature_img_used/augmented/shen_divided/images/\", \"./nature_img_used/augmented/shen_divided/labels/\", 4)\n",
    "\n",
    "# Divides the MA956 ODS dataset\n",
    "divide_images_and_labels(\"./nature_img_used/original_images/ma956/\", \"./nature_img_used/original_images/ma956/\", \"./nature_img_used/augmented/ma956_divided/images/\", \"./nature_img_used/augmented/ma956_divided/labels/\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Artery Dataset Classes (Function 3) <a class=\"anchor\" id=\"function3\"></a>\n",
    "\n",
    "**Purpose**  \n",
    "The `Artery` dataset labels define multiple classes of arteries. For the purpose of this study, all artery classes are changed to `0` (lines).\n",
    "\n",
    "**Parameters**  \n",
    "`label_dir`: The orginal label directory for the Artery dataset with differing Artery classes. \n",
    "\n",
    "`output_dir`: The output Artery label directory with a single Artery/line class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def combine_classes(label_dir, output_dir):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(label_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            input_path = os.path.join(label_dir, filename)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            with open(input_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            with open(output_path, 'w') as bbox_file:\n",
    "                for line in lines:\n",
    "                    numbers = line.split()\n",
    "                    numbers[0] = '0'  # Change the first number to 0\n",
    "                    new_line = ' '.join(numbers)  # Join the numbers back into a string\n",
    "                    bbox_file.write(new_line + '\\n')  # Write the modified line to the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_classes(\"./nature_img_used/original_images/artery/\", \"./nature_img_used/augmented/artery_combined/labels/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize Input Images (Function 4) <a class=\"anchor\" id=\"function4\"></a>\n",
    "\n",
    "**Purpose**  \n",
    "The `Artery` dataset images are 512x512. Function 1 will divide the `Shen et al.` images to a different size. This resizes them to 512x512 to provide a uniform image size for training. The `MA956` images are not resized, as the model performed better training on the original-sized images output from Function 1.\n",
    "\n",
    "**Parameters**  \n",
    "`divided_img_dir`: Directory with the divided Shen et al. images.\n",
    "\n",
    "`resized_img_dir`: Directory where resized images will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "def resize_images(divided_img_dir, resized_img_dir):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(resized_img_dir, exist_ok=True)\n",
    "\n",
    "    img_files = [f for f in os.listdir(divided_img_dir) if f.endswith('.tif') or f.endswith('.png') or f.endswith('.jpg')]\n",
    "    for img_file in img_files:\n",
    "        # Construct paths for the current image file\n",
    "        img_path = os.path.join(divided_img_dir, img_file)\n",
    "        output_img_path = os.path.join(resized_img_dir, img_file)\n",
    "  \n",
    "        # Read image\n",
    "        image = cv2.imread(img_path)\n",
    "        # Resize image to 512x512 if size is not already 512x512\n",
    "        if image.shape[:2] != (512, 512):\n",
    "            image = cv2.resize(image, (512, 512))\n",
    "\n",
    "        # Write resized image\n",
    "        cv2.imwrite(output_img_path, image)\n",
    "\n",
    "    print(\"Images are resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_images(\"./nature_img_used/augmented/shen_divided/images/\", \"./nature_img_used/augmented/shen_resized/images/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Training and Validation Sets (Function 5) <a class=\"anchor\" id=\"function5\"></a>\n",
    "\n",
    "**Purpose**  \n",
    "Randomly splits dataset images and corresponding labels into training and validation directories for model training. This is also used to match the number of `Artery` images to those of the post-processed `Shen et al.` ones. From testing, a train-val split of 80-20 was optimal for the Artery and Shen Model, while 90-10 performed better for the `MA956` model.\n",
    "\n",
    "\n",
    "**Parameters**  \n",
    "`img_dir`: The image directory of the full dataset.  \n",
    "\n",
    "`label_dir`: The label directory of the full dataset.  \n",
    "\n",
    "`train_img_dir`: The output directory to save the training images.\n",
    "\n",
    "`val_img_dir`: The output directory to save the validation images.\n",
    "\n",
    "`train_label_dir`: The output directory to save the training labels.\n",
    "\n",
    "`val_label_dir`: The output directory to save the validation labels.\n",
    "\n",
    "`val_ratio`: The ratio of images/labels to be used for validation to the total number in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_train_val(img_dir, label_dir, train_img_dir, val_img_dir, train_label_dir, val_label_dir, val_ratio):\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(train_img_dir, exist_ok=True)\n",
    "    os.makedirs(val_img_dir, exist_ok=True)\n",
    "    os.makedirs(train_label_dir, exist_ok=True)\n",
    "    os.makedirs(val_label_dir, exist_ok=True)\n",
    "    \n",
    "    img_files = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir) if filename.endswith('.tif') or filename.endswith('.png') or filename.endswith('.jpg')]\n",
    "    label_files = [os.path.join(label_dir, filename) for filename in os.listdir(label_dir) if filename.endswith('.txt')]\n",
    "\n",
    "    # Sort the filenames to ensure correspondence between images and labels\n",
    "    img_files.sort()\n",
    "    label_files.sort()\n",
    "\n",
    "    # Perform train-test split using sklearn.model_selection.\n",
    "    train_imgs, val_imgs, train_labels, val_labels = train_test_split(img_files, label_files, test_size=val_ratio, random_state=42)\n",
    "\n",
    "    # Copy train images to the training directory\n",
    "    for img_path in train_imgs:\n",
    "        img_filename = os.path.basename(img_path)\n",
    "        shutil.copy(img_path, os.path.join(train_img_dir, img_filename))\n",
    "\n",
    "    # Copy train labels to the training directory\n",
    "    for label_path in train_labels:\n",
    "        label_filename = os.path.basename(label_path)\n",
    "        shutil.copy(label_path, os.path.join(train_label_dir, label_filename))\n",
    "\n",
    "    # Copy val images to the validation directory\n",
    "    for img_path in val_imgs:\n",
    "        img_filename = os.path.basename(img_path)\n",
    "        shutil.copy(img_path, os.path.join(val_img_dir, img_filename))\n",
    "\n",
    "    # Copy val labels to the validation directory\n",
    "    for label_path in val_labels:\n",
    "        label_filename = os.path.basename(label_path)\n",
    "        shutil.copy(label_path, os.path.join(val_label_dir, label_filename))\n",
    "\n",
    "    # Check the lengths of the training and testing sets\n",
    "    print(\"Training set size:\", len(train_imgs))\n",
    "    print(\"Validation set size:\", len(val_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only need 39 artery images to match the 39 from Shen et al. post-divide\n",
    "split_train_val(\"./nature_img_used/original_images/artery/\", \"./nature_img_used/augmented/artery_combined/labels/\", \n",
    "                \"./nature_img_used/augmented/artery_extra/\", \"./nature_img_used/augmented/artery_used/images/\",\n",
    "               \"./nature_img_used/augmented/artery_extra/\", \"./nature_img_used/augmented/artery_used/labels/\", 0.13)\n",
    "\n",
    "\n",
    "# Set up training and validation directories for the artery/Shen model\n",
    "split_train_val(\"./nature_img_used/augmented/shen_resized/images/\", \"./nature_img_used/augmented/shen_divided/labels/\", \n",
    "                \"./nature_img_used/post-processed_images/artery_shen_model/images/train/\", \"./nature_img_used/post-processed_images/artery_shen_model/images/val/\",\n",
    "               \"./nature_img_used/post-processed_images/artery_shen_model/labels/train/\", \"./nature_img_used/post-processed_images/artery_shen_model/labels/val/\", 0.2)\n",
    "\n",
    "split_train_val(\"./nature_img_used/augmented/artery_used/images/\", \"./nature_img_used/augmented/artery_used/labels/\", \n",
    "                \"./nature_img_used/post-processed_images/artery_shen_model/images/train/\", \"./nature_img_used/post-processed_images/artery_shen_model/images/val/\",\n",
    "               \"./nature_img_used/post-processed_images/artery_shen_model/labels/train/\", \"./nature_img_used/post-processed_images/artery_shen_model/labels/val/\", 0.2)\n",
    "\n",
    "# Set up training and validation directories for models a and b\n",
    "split_train_val(\"./nature_img_used/augmented/ma956_divided/images/\", \"./nature_img_used/augmented/ma956_divided/labels/\", \n",
    "                \"./nature_img_used/post-processed_images/models_a_b/images/train/\", \"./nature_img_used/post-processed_images/models_a_b/images/val/\",\n",
    "               \"./nature_img_used/post-processed_images/models_a_b/labels/train/\", \"./nature_img_used/post-processed_images/models_a_b/labels/val/\", 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models A and B (Function 6) <a class=\"anchor\" id=\"function6\"></a>\n",
    "\n",
    "**Purpose**  \n",
    "Trains the models on the provided images with the same parameters used in the paper.   \n",
    "\n",
    "**Instructions**\n",
    "\n",
    "This function makes use of the `nature_models.yaml` file found in `nature_img_used/nature_models.yaml`. Users will need to specify their working directory (i.e. where they unzipped the `nature_img_used.zip` file) for the `path` variable. The two different lines with the `path` variable correspond to the two different training datasets used in training models A and B, and the Artery and Shen Model used to pretrain `Model B`. The correct line will need to be uncommented and the other remain commented when training those respective models.\n",
    "\n",
    "**Parameters**  \n",
    "\n",
    "`base_model`: The training model. YOLO can train from scratch, pre-trained weights from MSCOCO, or weight files specified from previous runs.\n",
    "\n",
    "`imgsz`: The size of the images in the training data. \n",
    "\n",
    "`epochs`: The number of training passes over the entire training dataset. \n",
    "\n",
    "`batch`: The number of training images to be processed before updating model weights. \n",
    "\n",
    "`lr0`: The initial learning rate of the model, which affects how much model weights can be updated after each image batch. \n",
    "\n",
    "`patience`: The number of epochs with no model improvements to wait before terminating the training. \n",
    "\n",
    "`project`: The output path for model runs.  \n",
    "\n",
    "\n",
    "\n",
    "**YOLOv8 Training Parameters**  \n",
    "\n",
    "`freeze`: The number of model layers whose weights are frozen for transfer learning.\n",
    "\n",
    "`verbose`: Enabling provides detailed logs and progress updates during training.  \n",
    "\n",
    "`plots`: Enabling outputs plots of training and validation metrics and prediction examples.   \n",
    "\n",
    "`amp` and `half`: Enabling can reduce memory usage and speed up training with minimal impact on accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the path to the YAML file\n",
    "yaml_path = \"./nature_img_used/nature_models.yaml\"\n",
    "\n",
    "def train_model(base_model, imgsz, epochs, batch, lr0, patience, project):\n",
    "    # Load the model\n",
    "    model = YOLO(base_model)\n",
    "    \n",
    "    # Generate Tensorboard graphs for monitoring during training\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True, write_images=True)\n",
    "    \n",
    "    model.train(\n",
    "        data=yaml_path, \n",
    "        imgsz=imgsz,\n",
    "        epochs=epochs,\n",
    "        batch=batch,\n",
    "        lr0=lr0,\n",
    "        patience=patience,\n",
    "        project=project,\n",
    "        freeze=1,\n",
    "        verbose=True,\n",
    "        amp=True,\n",
    "        plots=True,\n",
    "        half=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model('yolov8x-seg.pt', 640, 100, 32, 0.01, 100, \"./model-a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model('yolov8x-seg.pt', 512, 200, 16, 0.001, 50, \"./artery_shen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model('./artery_shen/train/weights/best.pt', 640, 100, 32, 0.01, 50, \"./model-b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with Models (Function 7)  <a class=\"anchor\" id=\"function7\"></a>\n",
    "\n",
    "\n",
    "**Purpose**  \n",
    "Predicts on the test images using the trained models and outputs mask label files.  \n",
    "\n",
    "**Parameters**  \n",
    "`model_weights_path`: The path to the best weights of the trained models. \n",
    "\n",
    "`test_img_dir`: The test image directory.   \n",
    "\n",
    "`project`: The output path for model predictions.\n",
    "\n",
    "`conf`: The minimum confidence threshold of predictions to output.\n",
    "\n",
    "**YOLOv8 Prediction Parameters**  \n",
    "\n",
    "`max_det`: The maximum dislocation instances detected.\n",
    "\n",
    "`iou`: The maximum Intersection Over Union (IoU) threshold between different predictions.\n",
    "\n",
    "`show_labels`: Disabling removes text labels for every prediction instance in output images.\n",
    "\n",
    "`show_boxes`: Disabling removes bounding boxes for every prediction instance in output images, and leaves only the segmentations. \n",
    "\n",
    "`save`: Enabling saves the prediction images.\n",
    "\n",
    "`save_txt`: Enabling saves the predictions in a .txt file of YOLO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import glob\n",
    "\n",
    "def predict(model_weights_path, test_img_dir, project, conf):\n",
    "    model = YOLO(model_weights_path)\n",
    "    \n",
    "    #Specify image directory model predicts on\n",
    "    test_img_dir = glob.glob(test_img_dir) \n",
    "\n",
    "    for img in test_img_dir:\n",
    "        model.predict(\n",
    "            source=img, \n",
    "            project=project, \n",
    "            conf=conf, \n",
    "            max_det=2500, \n",
    "            iou=0.7,\n",
    "            show_labels=False, \n",
    "            show_boxes=False, \n",
    "            save=True, \n",
    "            save_txt=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"./model-a/train/weights/best.pt\", \"./nature_img_used/test_img/images/\", \"./model-a/\", 0.04)\n",
    "predict(\"./model-b/train/weights/best.pt\", \"./nature_img_used/test_img/images/\", \"./model-b/\", 0.04)\n",
    "\n",
    "# Predict on Shen et al.'s images\n",
    "predict(\"./model-a/train/weights/best.pt\", \"./nature_img_used/shen_test/images/\", \"./model-a/\", 0.04)\n",
    "predict(\"./model-b/train/weights/best.pt\", \"./nature_img_used/shen_test/images/\", \"./model-b/\", 0.04) \n",
    "\n",
    "# Predict on HfAl with lower confidence threshold\n",
    "predict(\"./model-b/train/weights/best.pt\", \"./nature_img_used/hfal/\", \"./model-b/\", 0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Mask Images (Function 8) <a class=\"anchor\" id=\"function8\"></a>\n",
    "\n",
    "**Purpose**  \n",
    "Plots the masks from the label text files. Can do YOLO format as well as `Shen et al.`'s bounding boxes. Can overlay the masks on top of the original image, or create binary mask images of just the masks on a black background for metric calculation.  \n",
    "\n",
    "\n",
    "**Parameters**  \n",
    "`img_dir`: The unlabeled image directory.  \n",
    "\n",
    "`label_dir`: The label directory. The labels can be either the ground truth or model predictions, and must be in YOLO or Shen et al.'s format depending on the `label_format` specified.  \n",
    "\n",
    "`output_dir`: The output directory to save the mask images.\n",
    "\n",
    "`mode`: `overlay` (default) or `metric` depending on whether you want to overlay the masks on the original images, or plot just the binary masks for metric calculation, respectively.\n",
    "\n",
    "`label_format`: `yolo` (default) or `shen` depending on whether the label files provided are in YOLO format or `Shen et al.`'s format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def plot_masks(img_dir, label_dir, output_dir, mode='overlay', label_format='yolo'):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define colors for masks\n",
    "    line_color = (0, 255, 0)  # Green for lines\n",
    "    loop_color = (0, 0, 255)   # Red for loops\n",
    "    bb_color = (255, 255, 0)  # Cyan for Shen et al.'s bounding boxes\n",
    "\n",
    "    # Get a list of all image files in the directory\n",
    "    img_files = [f for f in os.listdir(img_dir) if f.endswith('.tif') or f.endswith('.png') or f.endswith('.jpg')]\n",
    "\n",
    "    # Process each image file\n",
    "    for img_file in img_files:\n",
    "        # Construct paths for the current image and label file\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        label_path = os.path.join(label_dir, f'{os.path.splitext(img_file)[0]}.txt')\n",
    "        output_img_path = os.path.join(output_dir, img_file)\n",
    "\n",
    "        # Check if the image and label files exist\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Image file '{img_path}' not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Label file '{label_path}' not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Read the image\n",
    "        img = cv2.imread(img_path)\n",
    "        h, w = img.shape[:2]  # Get height and width of the image\n",
    "\n",
    "        if mode != 'metric' and label_format != 'shen':  # Essentially just for YOLO format overlays but make that the default\n",
    "            # Create two blank mask images with fully transparent backgrounds - one for each label to combine later\n",
    "            line_mask = np.zeros((h, w, 4), dtype=np.uint8)  # 4 channels for RGBA (Red, Green, Blue, Alpha)\n",
    "            loop_mask = np.zeros((h, w, 4), dtype=np.uint8)\n",
    "        else:\n",
    "            # Create two blank black images - one for each label to combine later\n",
    "            line_mask = np.zeros((h, w, 3), dtype=np.uint8)  # 3 channels for RGBA (Red, Green, Blue)\n",
    "            loop_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "        # Read labels from the corresponding label file\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Loop though all masks and plot them\n",
    "        for line in lines:\n",
    "            data = line.strip().split(' ')\n",
    "            points = list(map(float, data[1:]))\n",
    "            \n",
    "            if label_format == 'shen':\n",
    "                y_min, x_min, y_max, x_max = map(int, points)  # This is the format Shen et al.'s labels used\n",
    "                if mode == 'metric':\n",
    "                    cv2.rectangle(loop_mask, (x_min, y_min), (x_max, y_max), color=loop_color, thickness=-1)\n",
    "                else:  # overlay\n",
    "                    cv2.rectangle(loop_mask, (x_min, y_min), (x_max, y_max), color=bb_color, thickness=4)\n",
    "            \n",
    "            else:  # YOLO format\n",
    "                class_id = int(data[0])\n",
    "                num_points = len(points) // 2\n",
    "                if num_points < 3:  # Need at least 3 points to define a polygon\n",
    "                    continue\n",
    "                polygon = np.array(points, dtype=np.float32).reshape(-1, 2)\n",
    "                polygon[:, 0] *= w\n",
    "                polygon[:, 1] *= h\n",
    "                polygon = polygon.astype(np.int32)\n",
    "\n",
    "                if mode == 'metric':\n",
    "                    if class_id == 0:\n",
    "                        cv2.fillPoly(line_mask, [polygon], color=line_color)\n",
    "                    elif class_id == 1:\n",
    "                        cv2.fillPoly(loop_mask, [polygon], color=loop_color)\n",
    "                else:  # overlay\n",
    "                    if class_id == 0:\n",
    "                        cv2.fillPoly(line_mask, [polygon], color=line_color + (100,))  # Adjust alpha value (100 for semi-transparency)\n",
    "                    elif class_id == 1:\n",
    "                        cv2.fillPoly(loop_mask, [polygon], color=loop_color + (100,))\n",
    "                \n",
    "        \n",
    "        # Combine the mask images into one, so that any overlap shows up as both classes\n",
    "        mask = cv2.add(line_mask, loop_mask)\n",
    "\n",
    "        if mode == 'metric':\n",
    "            output = mask\n",
    "        else:\n",
    "            if label_format == 'shen':\n",
    "                # Plot bounding boxes on Shen et al. images, since they are supposed to be solid this is a hacky way to not blend the other pixel colors\n",
    "                loop_grayscale = cv2.cvtColor(loop_mask,cv2.COLOR_BGR2GRAY)\n",
    "                mask = cv2.threshold(loop_grayscale, 10, 255, cv2.THRESH_BINARY)[1]\n",
    "                img_background = cv2.bitwise_and(img, img, mask = cv2.bitwise_not(mask))\n",
    "                output = cv2.add(img_background, loop_mask)\n",
    "            else:\n",
    "                # Overlay the mask on the original image using the mask as an alpha channel\n",
    "                img[:, :, :3] = cv2.addWeighted(img, 1, mask[:, :, :3], 0.3, 0)\n",
    "                output = img\n",
    "\n",
    "        # Save the overlay image with the masks and polylines\n",
    "        cv2.imwrite(output_img_path, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlaid masks to be used as paper figures\n",
    "# Ground truths\n",
    "plot_masks(img_dir=\"./nature_img_used/test_img/images/\", label_dir=\"./nature_img_used/test_img/labels/\", output_dir=\"./paper_masks/gt/\", mode='overlay', label_format='yolo')\n",
    "\n",
    "# Models A and B\n",
    "plot_masks(img_dir=\"./nature_img_used/test_img/images/\", label_dir=\"./model-a/predict/labels/\", output_dir=\"./paper_masks/a/\", mode='overlay', label_format='yolo')\n",
    "plot_masks(img_dir=\"./nature_img_used/test_img/images/\", label_dir=\"./model-b/predict/labels/\", output_dir=\"./paper_masks/b/\", mode='overlay', label_format='yolo')\n",
    "\n",
    "# Models A and B on Shen et al.'s images\n",
    "plot_masks(img_dir=\"./nature_img_used/shen_test/images/\", label_dir=\"./model-a/predict2/labels/\", output_dir=\"./paper_masks/a/\", mode='overlay', label_format='yolo')\n",
    "plot_masks(img_dir=\"./nature_img_used/shen_test/images/\", label_dir=\"./model-b/predict2/labels/\", output_dir=\"./paper_masks/b/\", mode='overlay', label_format='yolo')\n",
    "\n",
    "# Masks for HfAl image\n",
    "plot_masks(img_dir=\"./nature_img_used/hfal/\", label_dir=\"./model-b/predict3/labels/\", output_dir=\"./paper_masks/b/\", mode='overlay', label_format='yolo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shen et al.'s bounding boxes\n",
    "plot_masks(img_dir=\"./nature_img_used/shen_test/images/\", label_dir=\"./nature_img_used/shen_test/labels/\", output_dir=\"./paper_masks/shen/\", mode='overlay', label_format='shen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary mask images for metric calculation\n",
    "# Ground truths\n",
    "plot_masks(img_dir=\"./nature_img_used/test_img/images/\", label_dir=\"./nature_img_used/test_img/labels/\", output_dir=\"./binary_masks/gt/\", mode='metric', label_format='yolo')\n",
    "\n",
    "# Models A and B\n",
    "plot_masks(img_dir=\"./nature_img_used/test_img/images/\", label_dir=\"./model-a/predict/labels/\", output_dir=\"./binary_masks/a/\", mode='metric', label_format='yolo')\n",
    "plot_masks(img_dir=\"./nature_img_used/test_img/images/\", label_dir=\"./model-b/predict/labels/\", output_dir=\"./binary_masks/b/\", mode='metric', label_format='yolo')\n",
    "\n",
    "# Models A and B on Shen et al.'s images\n",
    "plot_masks(img_dir=\"./nature_img_used/shen_test/images/\", label_dir=\"./model-a/predict2/labels/\", output_dir=\"./binary_masks/a/\", mode='metric', label_format='yolo')\n",
    "plot_masks(img_dir=\"./nature_img_used/shen_test/images/\", label_dir=\"./model-b/predict2/labels/\", output_dir=\"./binary_masks/b/\", mode='metric', label_format='yolo')\n",
    "\n",
    "# Shen et al.'s bounding boxes\n",
    "plot_masks(img_dir=\"./nature_img_used/shen_test/images/\", label_dir=\"./nature_img_used/shen_test/labels/\", output_dir=\"./binary_masks/shen/\", mode='metric', label_format='shen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Calculate Metrics from Binary Masks (Function 9) <a class=\"anchor\" id=\"function9\"></a>\n",
    "**Purpose**  \n",
    "Calculates the Precision, Recall, F1 Score, and IOU metrics from binary mask images using boolean logic.\n",
    "\n",
    "**Parameters**  \n",
    "\n",
    "`gt_dir`: The directory containing ground truth binary mask images. \n",
    "\n",
    "`prediction_dir`: The directory containing model-predicted binary mask images. \n",
    "\n",
    "`model_name`: Name of the model predicted with.  \n",
    "\n",
    "`dislocation_class`: `loops` or `lines`, corresponding to which class metrics should be outputted for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def calculate_metrics(gt_dir, prediction_dir, model_name, dislocation_class):\n",
    "    # Sets the dimension of rbg pixels to look at depending on whether we want metrics for loops or lines\n",
    "    if dislocation_class == 'lines':\n",
    "        rbg_dim = 1\n",
    "    else:\n",
    "        rbg_dim = 0\n",
    "    \n",
    "    img_files = [f for f in os.listdir(gt_dir) if f.endswith('.tif') or f.endswith('.png') or f.endswith('.jpg')]\n",
    "    for img_name in img_files:\n",
    "        GT_path = os.path.join(gt_dir, img_name)\n",
    "        pred_path = os.path.join(prediction_dir, img_name)\n",
    "        \n",
    "        # Read in images\n",
    "        img_GT = Image.open(GT_path)\n",
    "        img_pred = Image.open(pred_path)\n",
    "        \n",
    "        # Convert pixel values to binary based on whether they contain any non-zero value\n",
    "        gt_bbox = np.array(img_GT) > 0\n",
    "        pred_mask = np.array(img_pred) > 0\n",
    "\n",
    "        gt_bbox = gt_bbox[:,:,rbg_dim]\n",
    "        pred_mask = pred_mask[:,:,rbg_dim]\n",
    "        \n",
    "        # Calculate true positives (tp), false positives (fp), and false negatives (fn) using boolean logical operators\n",
    "        tp = np.sum(np.logical_and(gt_bbox, pred_mask))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(gt_bbox), pred_mask))\n",
    "        fn = np.sum(np.logical_and(gt_bbox, np.logical_not(pred_mask)))\n",
    "        \n",
    "        # Calculate IOU, precision, recall, and f1 score, add 1e-9 so we don't have a divide by zero\n",
    "        precision = tp / (tp + fp + 1e-9)\n",
    "        recall = tp / (tp + fn + 1e-9)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "        iou = tp / (tp + fp + fn + 1e-9)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"\\nThe precision of predicted \" + dislocation_class + \" from \" + model_name + \" for sample \" + img_name + \" is: \" + str(precision))\n",
    "        print(\"The recall of predicted \" + dislocation_class + \" from \" + model_name + \" for sample \" + img_name + \" is: \" + str(recall))\n",
    "        print(\"The F1 Score of predicted \" + dislocation_class + \" from \" + model_name + \" for sample \" + img_name + \" is: \" + str(f1_score))\n",
    "        print(\"The IOU of predicted \" + dislocation_class + \" from \" + model_name + \" for sample \" + img_name + \" is: \" + str(iou))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(\"./binary_masks/gt/\", \"./binary_masks/a/\", \"Model A\", \"loops\")\n",
    "calculate_metrics(\"./binary_masks/gt/\", \"./binary_masks/a/\", \"Model A\", \"lines\")\n",
    "calculate_metrics(\"./binary_masks/gt/\", \"./binary_masks/b/\", \"Model B\", \"loops\")\n",
    "calculate_metrics(\"./binary_masks/gt/\", \"./binary_masks/b/\", \"Model B\", \"lines\")\n",
    "calculate_metrics(\"./binary_masks/shen/\", \"./binary_masks/a/\", \"Model A\", \"loops\")\n",
    "calculate_metrics(\"./binary_masks/shen/\", \"./binary_masks/b/\", \"Model B\", \"loops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Noise Level of Images (Function 10) <a class=\"anchor\" id=\"function10\"></a>\n",
    "**Purpose**  \n",
    "Quantifies the noisiness of images, where higher values correspond to higher noise in an image.\n",
    "\n",
    "**Parameters**  \n",
    "\n",
    "`img_dir`: Directory of the images to calculate the noise level of.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def noise_level(img_dir):\n",
    "    img_files = [f for f in os.listdir(img_dir) if f.endswith('.tif') or f.endswith('.png') or f.endswith('.jpg')]\n",
    "    for img_name in img_files:\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        \n",
    "        # Load the image\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Calculate standard deviation of pixel values\n",
    "        std_dev = np.std(image)\n",
    "\n",
    "        print(f\"The standard deviation of pixel values of image {img_name} is {std_dev}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate noise levels of Shen et al. test images\n",
    "noise_level('./nature_img_used/shen_test/images/')\n",
    "\n",
    "# Calculate noise levels of MA956 test images\n",
    "noise_level('./nature_img_used/test_img/images/')\n",
    "\n",
    "# Calculate noise levels of HfAl image\n",
    "noise_level('./nature_img_used/hfal/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "yolov8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
